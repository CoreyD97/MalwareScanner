package com.coreyd97.c45;

import javax.xml.crypto.Data;
import java.util.*;

public class C45 {

    final String classificationA;
    final String classificationB;
    int minInstances;
    List<DataElement> trainingElements;
    List<Feature> features;
    double splitValue;
    double bestSplitValue;

    public TreeNode root;

    //TODO For report, include info on simplifying sparce feature matrix.
    public C45(String classificationA, String classificationB, int minInstances){
        this.classificationA = classificationA;
        this.classificationB = classificationB;
        this.minInstances = minInstances;
    }

    public void addTrainingElements(List<DataElement> trainingElements){
        if(this.trainingElements == null) this.trainingElements = trainingElements;
        else this.trainingElements.addAll(trainingElements);
    }

    public void addFeatures(List<Feature> features){
        if(this.features == null) this.features = features;
        else this.features.addAll(features);
    }

    public void createTree(){
        root = decisionTreeLearning(trainingElements, features, trainingElements);
    }

    public void orderFeatures(){
        List<Feature> remaining = new ArrayList<>(this.features);
        while (remaining.size() > 175){
            Feature bestFeature = bestFeature(this.trainingElements, remaining);
            System.out.println(bestFeature.name + " - "  + entropyGain(trainingElements, bestFeature));
            remaining.remove(bestFeature);
        }
    }

    public void test(List<DataElement> tests){
        System.out.println("\n\nPREDICTED : ACTUAL");
        int correct = 0;
        for (DataElement test : tests) {
            String predicted = root.classify(test);
            System.out.println(predicted + " " + test.classification);
            if(predicted.equals(test.classification))
                correct++;
        }
        System.out.println("\nCorrect: " + correct);
        System.out.println("Total: " + tests.size());
        double hitRate = ((double) correct)/tests.size();
        System.out.println("Hit Rate: " + (hitRate*100) + "%");
    }

    //TODO replace with non-pasted version
    private TreeNode decisionTreeLearning(List<DataElement> elements,
                                      List<Feature> features, List<DataElement> parentInstances) {
        if (elements.isEmpty())
            return new TreeNode(majorityClassification(parentInstances), true);
        if (elements.size() < minInstances)
            return new TreeNode(majorityClassification(elements), true);
        if (allSameClass(elements))
            return new TreeNode(elements.get(0).classification, true);

        // Returns null if no more features / positive gain features
        Feature bestAttribute = bestFeature(elements, features);
        if (bestAttribute == null) {
            return new TreeNode(majorityClassification(elements), true);
        }
        TreeNode root = new TreeNode(bestAttribute.name, false);

        // Partition instances based on best attribute
        LinkedHashMap<String, List<DataElement>> subsets = new LinkedHashMap();
        double currentSplitValue = bestSplitValue;
        if(bestAttribute.isBoolean){
            root.isBoolean = true;
            subsets.put(String.valueOf(true), new ArrayList<>());
            subsets.put(String.valueOf(false), new ArrayList<>());
            for (DataElement element : elements) {
                subsets.get(String.valueOf(element.labels.contains(bestAttribute.name))).add(element);
            }
        } else if (bestAttribute.isCategoric) {
            root.isCategoric = true;
            for (String value : bestAttribute.values)
                subsets.put(value, new ArrayList<DataElement>());
            for (DataElement instance : elements) {
                List<DataElement> subset = subsets.get(instance.features.get(bestAttribute.name));
                subset.add(instance);
            }
        } else {
            root.isCategoric = false;
            root.value = String.valueOf(currentSplitValue);
            List<DataElement> leq = new ArrayList<>();
            List<DataElement> gtr = new ArrayList<>();
            for (DataElement element : elements) {
                double value = Double.parseDouble(element.features.get(bestAttribute.name));
                if (value <= currentSplitValue)
                    leq.add(element);
                else
                    gtr.add(element);
            }
            subsets.put("leq", leq);
            subsets.put("gtr", gtr);
        }

        // Iterate through subsets (children) of node
        for(String key: subsets.keySet()) {
            // Copy attributes list and remove bestAttribute from copy
            List<Feature> remainingAttributes = new ArrayList<Feature>();
            remainingAttributes.addAll(features);
            if (bestAttribute.isCategoric)
                remainingAttributes.remove(bestAttribute);
            // Calculate next node in subset using the remaining attributes
            TreeNode child = decisionTreeLearning(subsets.get(key), remainingAttributes, elements);
            if (bestAttribute.isCategoric) {
                child.value = key;
            }

            root.addChild(child);
        }
        return root;
    }

    //Returns best attribute to split. Null if no splits or positive gain attributes.
    private Feature bestFeature(List<DataElement> elements, List<Feature> features){
        double maxGain = Double.MIN_VALUE;
        Feature bestFeature = null;
        for (Feature feature : features) {
            double gain = entropyGain(elements, feature);
            if(gain > maxGain){
                maxGain = gain;
                bestFeature = feature;
                bestSplitValue = splitValue;
            }
        }
        if(maxGain <= 0) return null;
        return bestFeature;
    }

    private double entropyGain(List<DataElement> elements, Feature feature){
        if(elements.isEmpty()) return 0.0;
        if(feature.isCategoric){
            return calculateEntropy(elements) - featureEntropy(elements, feature);
        }else{
            ArrayList<Double> featureValues = new ArrayList<>();
            ArrayList<Double> splitPoints = new ArrayList<>();
            for (DataElement element : elements) {
                featureValues.add(Double.parseDouble(element.features.get(feature.name)));
            }
            Collections.sort(featureValues);
            for (int i = 0; i < featureValues.size()-1; i++) {
                splitPoints.add((featureValues.get(i) + featureValues.get(i+1))/2);
            }

            double maxGain = Double.MIN_VALUE;
            splitValue = Double.MIN_VALUE;
            for (Double splitPoint : splitPoints) {
                double gain = calculateEntropy(elements) - featureEntropy(elements, feature, splitPoint);
                if(gain > maxGain){
                    maxGain = gain;
                    splitValue = splitPoint;
                }
            }
            return maxGain;
        }
    }


    private boolean allSameClass(List<DataElement> elements){
        String classification = elements.get(0).classification;
        for (int i = 1; i < elements.size(); i++) {
            if(!elements.get(i).classification.equals(classification)) return false;
        }
        return true;
    }

    private String majorityClassification(List<DataElement> elements){
        int countA = 0;
        for (DataElement element : elements) {
            if(element.classification.equals(this.classificationA)){
                countA++;
            }
        }
        if(countA >= elements.size() - countA) return classificationA;
        else return classificationB;
    }

    private double calculateEntropy(List<DataElement> elements){
        if(elements.size() == 0) return 0.0;
        int totalClassA = 0;
        int totalClassB = 0;
        for (DataElement element : elements) {
            if(element.classification.equals(this.classificationA))
                totalClassA++;
            else
                totalClassB++;
        }
        double pA = ((double) totalClassA / (totalClassA+totalClassB));
        double pB = ((double) totalClassB / (totalClassA+totalClassB));
        return -((pA * log2(pA)) + (pB * log2(pB)));
    }

    private double log2(double x){
        if(x == 0.0) return 0.0;
        return (Math.log(x) / Math.log(2));
    }

    public double featureEntropy(List<DataElement> elements, Feature feature){
        if(elements.isEmpty()) return 0.0;
        if(feature.isBoolean){
            HashMap<Boolean, List<DataElement>> booleanSubset = new HashMap<>();
            booleanSubset.put(true, new ArrayList<>());
            booleanSubset.put(false, new ArrayList<>());

            for (DataElement element : elements) {
                //Add element to subset dependent on containing the feature.
                booleanSubset.get(element.labels.contains(feature.name)).add(element);
            }
            double pTrue = ((double) booleanSubset.get(true).size()) / elements.size();
            double pFalse = ((double) booleanSubset.get(false).size()) / elements.size();
            return (pTrue * calculateEntropy(booleanSubset.get(true)))
                    + (pFalse * calculateEntropy(booleanSubset.get(false)));

        }else {
            //Create map of possible values and elements with each value
            HashMap<String, List<DataElement>> elementsWithFeatureValue = new HashMap<>();
            for (String value : feature.values) {
                elementsWithFeatureValue.put(value, new ArrayList<>());
            }
            //Add elements to the corresponding value subset
            for (DataElement element : elements) {
                String featureValue = element.features.get(feature.name);
                List<DataElement> subset = elementsWithFeatureValue.get(featureValue);
                subset.add(element);
            }
            double entropy = 0.0;
            for (List<DataElement> subset : elementsWithFeatureValue.values()) {
                double p = ((double) subset.size()) / elements.size();
                entropy += (p * this.calculateEntropy(subset));
            }
            return entropy;
        }
    }

    private double featureEntropy(List<DataElement> elements, Feature feature, double split){
        ArrayList<DataElement> LT = new ArrayList<>();
        ArrayList<DataElement> GEQ = new ArrayList<>();
        for (DataElement element : elements) {
            if(Double.parseDouble(element.features.get(feature.name)) >= split)
                GEQ.add(element);
            else
                LT.add(element);
        }
        double pLT = ((double) LT.size()) / elements.size();
        double pGEQ = ((double) GEQ.size()) / elements.size();
        return (pLT * calculateEntropy(LT)) + (pGEQ * calculateEntropy(GEQ));
    }

}
