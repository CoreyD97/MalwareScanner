package com.coreyd97.c45;

import java.util.*;
import java.util.concurrent.*;
import java.util.logging.Logger;

public class C45 {

    final String classificationA;
    final String classificationB;
    int minInstances;
    List<DataElement> trainingElements;
    List<Feature> features;

    public TreeNode root;

    private ExecutorService executor;

    //TODO For report, include info on simplifying sparce feature matrix.
    public C45(String classificationA, String classificationB, int minInstances){
        this.classificationA = classificationA;
        this.classificationB = classificationB;
        this.minInstances = minInstances;

        this.executor = Executors.newFixedThreadPool(10);
    }

    public void addTrainingElements(List<DataElement> trainingElements){
        if(this.trainingElements == null) this.trainingElements = trainingElements;
        else this.trainingElements.addAll(trainingElements);
    }

    public void addFeatures(List<Feature> features){
        if(this.features == null) this.features = features;
        else this.features.addAll(features);
    }

    public void createTree(){
        root = decisionTreeLearning(trainingElements, features, trainingElements);
        root.simplify();
    }

    public void test(List<DataElement> tests){
        System.out.println("\n\nPREDICTED : ACTUAL");
        int correct = 0;
        for (DataElement test : tests) {
            String predicted = root.classify(test);
            System.out.println(predicted + " " + test.classification);
            if(predicted.equals(test.classification))
                correct++;
        }
        System.out.println("\nCorrect: " + correct);
        System.out.println("Total: " + tests.size());
        double hitRate = ((double) correct)/tests.size();
        System.out.println("Hit Rate: " + (hitRate*100) + "%");
    }

    private TreeNode decisionTreeLearning(List<DataElement> elements,
                                      List<Feature> features, List<DataElement> parentInstances) {
        if (elements.isEmpty())
            return new TreeNode(majorityClassification(parentInstances), true);
        if (elements.size() < minInstances)
            return new TreeNode(majorityClassification(elements), true);
        if (allSameClass(elements))
            return new TreeNode(elements.get(0).classification, true);

        // Returns null if no more features / positive gain features
        // Returns null if no more features / positive gain features
//        FeatureEntropy bestAttribute = bestFeature(elements, features);
        FeatureEntropy bestAttribute = C45.bestFeature(executor, elements, features, this.classificationA);
        if (bestAttribute == null) {
            return new TreeNode(majorityClassification(elements), true);
        }
        TreeNode root = new TreeNode(bestAttribute.feature.name, false);

        // Partition instances based on best attribute
        LinkedHashMap<String, List<DataElement>> subsets = new LinkedHashMap();
        double currentSplitValue = bestAttribute.splitValue;
        if(bestAttribute.feature.isBoolean){
            root.isBoolean = true;
            subsets.put(String.valueOf(true), new ArrayList<>());
            subsets.put(String.valueOf(false), new ArrayList<>());
            for (DataElement element : elements) {
                subsets.get(String.valueOf(element.labels.contains(bestAttribute.feature.name))).add(element);
            }
        } else if (bestAttribute.feature.isCategoric) {
            root.isCategoric = true;
            for (String value : bestAttribute.feature.values)
                subsets.put(value, new ArrayList<DataElement>());
            for (DataElement instance : elements) {
                List<DataElement> subset = subsets.get(instance.features.get(bestAttribute.feature.name));
                subset.add(instance);
            }
        } else {
            root.isCategoric = false;
            root.value = String.valueOf(currentSplitValue);
            List<DataElement> leq = new ArrayList<>();
            List<DataElement> gtr = new ArrayList<>();
            for (DataElement element : elements) {
                double value = Double.parseDouble(element.features.get(bestAttribute.feature.name));
                if (value <= currentSplitValue)
                    leq.add(element);
                else
                    gtr.add(element);
            }
            subsets.put("leq", leq);
            subsets.put("gtr", gtr);
        }

        // Iterate through subsets (children) of node
        for(String key: subsets.keySet()) {
            // Copy attributes list and remove bestAttribute from copy
            List<Feature> remainingAttributes = new ArrayList<Feature>();
            remainingAttributes.addAll(features);
            if (bestAttribute.feature.isCategoric)
                remainingAttributes.remove(bestAttribute.feature);
            // Calculate next node in subset using the remaining attributes
            TreeNode child = decisionTreeLearning(subsets.get(key), remainingAttributes, elements);
            if (bestAttribute.feature.isCategoric) {
                child.value = key;
            }

            root.addChild(child);
        }
        return root;
    }

    //Returns best attribute to split. Null if no splits or positive gain attributes.
    public static FeatureEntropy bestFeature(List<DataElement> elements, Double totalEntropy, List<Feature> features, String classificationA){
        double maxGain = Double.MIN_VALUE;
        FeatureEntropy bestFeature = null;
        for (Feature feature : features) {
            FeatureEntropy featureEntropy = new FeatureEntropy(feature, elements, totalEntropy, classificationA);
            FeatureEntropy.calculateEntropy(featureEntropy);
            double gain = featureEntropy.entropyGain;
            if(gain > maxGain){
                maxGain = gain;
                bestFeature = featureEntropy;
            }
        }
        if(maxGain <= 0){
            return null;
        }
        return bestFeature;
    }

    public static FeatureEntropy bestFeature(ExecutorService e, List<DataElement> elements, List<Feature> features, String classificationA){
        ArrayList<Future<FeatureEntropy>> futureList = new ArrayList<>();
        FeatureEntropy best = null;
        int groupSize = 500;
        final double totalEntropy = calculateEntropy(elements, classificationA);
        long start = System.currentTimeMillis();
        for (int i = 0; i < features.size(); i += groupSize) {
            int finalI = i;
            Future<FeatureEntropy> f = e.submit(new Callable<FeatureEntropy>() {
                @Override
                public FeatureEntropy call() throws Exception {
                    List<Feature> subset = features.subList(finalI, Math.min(finalI+groupSize, features.size()));
                    FeatureEntropy result = C45.bestFeature(elements, totalEntropy, subset, classificationA);
                    return result;
                }
            });
            futureList.add(f);
        }
        while(futureList.size() > 0){
            for(Iterator<Future<FeatureEntropy>> iterator = futureList.iterator(); iterator.hasNext(); ){
                Future<FeatureEntropy> future = iterator.next();
                if(future.isDone()){
                    try {
                        FeatureEntropy result = future.get();
                        if(best == null || (result != null && result.entropyGain > best.entropyGain)){
                            best = result;
                        }
                    } catch (InterruptedException | ExecutionException e1) {
                        e1.printStackTrace();
                    }
                    iterator.remove();
                }
            }
        }
        if(best == null) {
            System.out.println();
        }
        Logger.getLogger("BestFeature").info(features.size() + " - " + ((System.currentTimeMillis()-start)/60000) + " Best:" + (best == null ? "NULL" : best.feature.name));
        return best;
    }

    private boolean allSameClass(List<DataElement> elements){
        String classification = elements.get(0).classification;
        for (int i = 1; i < elements.size(); i++) {
            if(!elements.get(i).classification.equals(classification)) return false;
        }
        return true;
    }

    private String majorityClassification(List<DataElement> elements){
        int countA = 0;
        for (DataElement element : elements) {
            if(element.classification.equals(this.classificationA)){
                countA++;
            }
        }
        if(countA >= elements.size() - countA) return classificationA;
        else return classificationB;
    }

    public static double calculateEntropy(List<DataElement> elements, String classificationA){
        if(elements.size() == 0) return 0.0;
        int totalClassA = 0;
        int totalClassB = 0;
        for (DataElement element : elements) {
            if(element.classification.equals(classificationA))
                totalClassA++;
            else
                totalClassB++;
        }
        double pA = ((double) totalClassA / (totalClassA+totalClassB));
        double pB = ((double) totalClassB / (totalClassA+totalClassB));
        return -((pA * log2(pA)) + (pB * log2(pB)));
    }

    private static double log2(double x){
        if(x == 0.0) return 0.0;
        return (Math.log(x) / Math.log(2));
    }

    public static double featureEntropy(List<DataElement> elements, Feature feature, String classificationA){
        if(elements.isEmpty()) return 0.0;
        if(feature.isBoolean){
            HashMap<Boolean, List<DataElement>> booleanSubset = new HashMap<>();
            booleanSubset.put(true, new ArrayList<>());
            booleanSubset.put(false, new ArrayList<>());

            for (DataElement element : elements) {
                //Add element to subset dependent on containing the feature.
                booleanSubset.get(element.labels.contains(feature.name)).add(element);
            }
            double pTrue = ((double) booleanSubset.get(true).size()) / elements.size();
            double pFalse = ((double) booleanSubset.get(false).size()) / elements.size();
            return (pTrue * calculateEntropy(booleanSubset.get(true), classificationA))
                    + (pFalse * calculateEntropy(booleanSubset.get(false), classificationA));

        }else {
            //Create map of possible values and elements with each value
            HashMap<String, List<DataElement>> elementsWithFeatureValue = new HashMap<>();
            for (String value : feature.values) {
                elementsWithFeatureValue.put(value, new ArrayList<>());
            }
            //Add elements to the corresponding value subset
            for (DataElement element : elements) {
                String featureValue = element.features.get(feature.name);
                List<DataElement> subset = elementsWithFeatureValue.get(featureValue);
                subset.add(element);
            }
            double entropy = 0.0;
            for (List<DataElement> subset : elementsWithFeatureValue.values()) {
                double p = ((double) subset.size()) / elements.size();
                entropy += (p * calculateEntropy(subset, classificationA));
            }
            return entropy;
        }
    }

    static double featureEntropy(List<DataElement> elements, Feature feature, double split, String classificationA){
        ArrayList<DataElement> LT = new ArrayList<>();
        ArrayList<DataElement> GEQ = new ArrayList<>();
        for (DataElement element : elements) {
            if(Double.parseDouble(element.features.get(feature.name)) >= split)
                GEQ.add(element);
            else
                LT.add(element);
        }
        double pLT = ((double) LT.size()) / elements.size();
        double pGEQ = ((double) GEQ.size()) / elements.size();
        return (pLT * calculateEntropy(LT, classificationA)) + (pGEQ * calculateEntropy(GEQ, classificationA));
    }

}
